# Install with Conda
conda install -c conda-forge pyarrow

# Install PyArrow with Python
pip install pyarrow==0.15.0

# Install Py4j with Python
pip install py4j==0.10.9

# Install pyspark with Python
pip install pyspark==3.0.0


#Spark Session Configurations

import pyarrow
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("PandasUDF_with_PyArrow") \
    .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
    .config("spark.sql.execution.arrow.pyspark.fallback.enabled", "true") \
    .config("spark.sql.parquet.mergeSchema", "false") \
    .config("spark.hadoop.parquet.enable.summary-metadata", "false") \
    .getOrCreate()

# Spark Session Details
spark

# Spark Session All Assigned Configurations 
spark.sparkContext.getConf().getAll()

#Parquet to Arrow
import pandas as pd
import numpy as np

# Importing PyArrow Parquet
import pyarrow.parquet as pq

# Link to data: 
# https://www.kaggle.com/yassinealouini/m5-sales-hierarchy-dataset

path = 'dataset/dimension.parquet'

data_frame = pq.read_table(path).to_pandas()

#Parquet to Arrow with Pandas Dataframe
import pandas as pd
import numpy as np

import pyarrow as pa
import pyarrow.parquet as pq

pandas_df = pd.DataFrame(data={'column_1': [1, 2], 'column_2': [3, 4], 'column_3': [5, 6]})
table = pa.Table.from_pandas(pandas_df, preserve_index=True)
pq.write_table(table, 'pandas_dataframe.parquet')

# Calculating Script Processing Time
import time

# Processor Time
time.process_time()

# Wall-Clock Time
time.perf_counter()

time.time()

time.monotonic()


# PySpark Pandas UDFs (Vectorized UDFs)
# 1. SCALAR Pandas UDF

from pyspark.sql.functions import pandas_udf
from pyspark.sql import Window

dataframe = spark.createDataFrame(
    [(1, 5), (2, 7), (2, 8), (2, 10), (3, 18), (3, 22), (4, 36)],
    ("index", "weight"))

# The function definition and the UDF creation
@pandas_udf("int")
def weight_avg_udf(weight: pd.Series) -> float:
    return weight.mean()

dataframe.select(weight_avg_udf(dataframe['weight'])).show()

# 2. GROUPED_AGG Pandas UDF
# Aggregation Process on Pandas UDF
dataframe.groupby("index").agg(weight_avg_udf(dataframe['weight'])).show()

w = Window \
    .partitionBy('index') \
    .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)
    
# Print the windowed results
dataframe.withColumn('avg_weight', weight_avg_udf(dataframe['weight']).over(w)).show()

# 3. GROUPED_MAP Pandas UDF
import pandas as pd
import numpy as np

# Pandas DataFrame generation
pandas_dataframe = pd.DataFrame(np.random.rand(200, 4))

def weight_map_udf(pandas_dataframe):
    weight = pandas_dataframe.weight
    return pandas_dataframe.assign(weight=weight - weight.mean())

dataframe.groupby("index").applyInPandas(weight_map_udf, schema="index int, weight int").show()
